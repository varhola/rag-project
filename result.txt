Retrieval evaluation

EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'

Not taking into account the 65 questions with insufficient data for answers

Does it give matches that are part of the correct ones(gives correct context for an answer)
First similarity is a match: 338/572
With three most similar we have a match: 435/572
With five most similar we have a match: 475/572

Does it leave some of the correct labels unmatched (will have enought context for answer)
That we get all necessary ones with five most similar ones: 434/572
That we get all necessary ones with three most similar ones: 398/572

Model answering evaluation

Model is asked if "A is the answer to question B based on context C and D". 

Round 0
Correct with correct context: 246/572
Labeled correct with incorrect context: 53/572
Round 1
260/572
44/572
Round 2
251/572
48/572
Round 3
254/572
57/572
Round 4
252/572
60/572
Not correctly labeled with correct context in every round: [0, 1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 20, 21, 22, 24, 26, 40, 43, 44, 45, 46, 47, 51, 52, 55, 56, 57, 59, 60, 61, 79, 86, 88, 100, 137, 144, 146, 153, 155, 164, 174, 225, 228, 232, 236, 240, 241, 253, 254, 256, 264, 266, 268, 270, 283, 290, 294, 295, 297, 303, 340, 343, 344, 347, 348, 350, 353, 361, 363, 365, 373, 375, 388, 389, 390, 391, 412, 414, 415, 439, 440, 441, 444, 447, 458, 461, 463, 466, 467, 468, 469, 470, 471, 472, 473, 490, 491, 495, 498, 499, 501, 502, 517, 534, 544, 546, 566]

Round 0
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
44/572
Round 4
265/572
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
230/572
45/572
Round 8
256/572
38/572
Round 9
240/572
65/572

[3, 4, 6, 7, 8, 9, 17, 20, 24, 25, 26, 39, 40, 43, 44, 45, 46, 47, 49, 51, 52, 55, 57, 59, 61, 88, 146, 174, 254, 264, 270, 283, 284, 289, 292, 293, 294, 295, 326, 347, 350, 351, 353, 383, 388, 389, 390, 392, 412, 429, 439, 441, 445, 447, 456, 458, 459, 460, 461, 463, 466, 467, 471, 473, 493, 495, 498, 499, 501, 544]

Combined evaluation

Round: 0
Correctly labeled as true: 265/625
Round: 1
247/625
Round: 2
258/625
Round: 3
260/625
Round: 4
245/625
EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'

Not taking into account the 65 questions with insufficient data for answers

Does it give matches that are part of the correct ones(gives correct context for an answer)
First similarity is a match: 338/572
With three most similar we have a match: 435/572
With five most similar we have a match: 475/572

Does it leave some of the correct labels unmatched (will have enought context for answer)
That we get all necessary ones with five most similar ones: 434/572
That we get all necessary ones with three most similar ones: 398/572

Model answering evaluation

Model is asked if "A is the answer to question B based on context C and D". 

Round 0
Correct with correct context: 246/572
Labeled correct with incorrect context: 53/572
Round 1
260/572
44/572
Round 2
251/572
48/572
Round 3
254/572
57/572
Round 4
252/572
60/572
Not correctly labeled with correct context in every round: [0, 1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 20, 21, 22, 24, 26, 40, 43, 44, 45, 46, 47, 51, 52, 55, 56, 57, 59, 60, 61, 79, 86, 88, 100, 137, 144, 146, 153, 155, 164, 174, 225, 228, 232, 236, 240, 241, 253, 254, 256, 264, 266, 268, 270, 283, 290, 294, 295, 297, 303, 340, 343, 344, 347, 348, 350, 353, 361, 363, 365, 373, 375, 388, 389, 390, 391, 412, 414, 415, 439, 440, 441, 444, 447, 458, 461, 463, 466, 467, 468, 469, 470, 471, 472, 473, 490, 491, 495, 498, 499, 501, 502, 517, 534, 544, 546, 566]

Round 0
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
44/572
Round 4
265/572
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
230/572
45/572
Round 8
256/572
38/572
Round 9
240/572
65/572

[3, 4, 6, 7, 8, 9, 17, 20, 24, 25, 26, 39, 40, 43, 44, 45, 46, 47, 49, 51, 52, 55, 57, 59, 61, 88, 146, 174, 254, 264, 270, 283, 284, 289, 292, 293, 294, 295, 326, 347, 350, 351, 353, 383, 388, 389, 390, 392, 412, 429, 439, 441, 445, 447, 456, 458, 459, 460, 461, 463, 466, 467, 471, 473, 493, 495, 498, 499, 501, 544]

Combined evaluation

Round: 0
Correctly labeled as true: 265/625
Round: 1
247/625
Round: 2
258/625
Round: 3
260/625
Round: 4
245/625
EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'

Not taking into account the 65 questions with insufficient data for answers

Does it give matches that are part of the correct ones(gives correct context for an answer)
First similarity is a match: 338/572
With three most similar we have a match: 435/572
With five most similar we have a match: 475/572

Does it leave some of the correct labels unmatched (will have enought context for answer)
That we get all necessary ones with five most similar ones: 434/572
That we get all necessary ones with three most similar ones: 398/572

Model answering evaluation

Model is asked if "A is the answer to question B based on context C and D". 

Round 0
Correct with correct context: 246/572
Labeled correct with incorrect context: 53/572
Round 1
260/572
44/572
Round 2
251/572
48/572
Round 3
254/572
57/572
Round 4
252/572
60/572
Not correctly labeled with correct context in every round: [0, 1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 20, 21, 22, 24, 26, 40, 43, 44, 45, 46, 47, 51, 52, 55, 56, 57, 59, 60, 61, 79, 86, 88, 100, 137, 144, 146, 153, 155, 164, 174, 225, 228, 232, 236, 240, 241, 253, 254, 256, 264, 266, 268, 270, 283, 290, 294, 295, 297, 303, 340, 343, 344, 347, 348, 350, 353, 361, 363, 365, 373, 375, 388, 389, 390, 391, 412, 414, 415, 439, 440, 441, 444, 447, 458, 461, 463, 466, 467, 468, 469, 470, 471, 472, 473, 490, 491, 495, 498, 499, 501, 502, 517, 534, 544, 546, 566]

Round 0
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
235/572
52/572
Round 1
255/572
52/572
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
Round 2
285/572
47/572
Round 3
233/572
44/572
Round 4
265/572
44/572
Round 4
265/572
265/572
56/572
Round 5
240/572
49/572
Round 6
251/572
69/572
Round 7
230/572
45/572
Round 8
256/572
38/572
Round 9
240/572
65/572

[3, 4, 6, 7, 8, 9, 17, 20, 24, 25, 26, 39, 40, 43, 44, 45, 46, 47, 49, 51, 52, 55, 57, 59, 61, 88, 146, 174, 254, 264, 270, 283, 284, 289, 292, 293, 294, 295, 326, 347, 350, 351, 353, 383, 388, 389, 390, 392, 412, 429, 439, 441, 445, 447, 456, 458, 459, 460, 461, 463, 466, 467, 471, 473, 493, 495, 498, 499, 501, 544]

Combined evaluation
