{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969ec7c-144a-49f0-8b45-08471d4e8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DIR = \"...\\\\LiHua-World\" # set the path\n",
    "OUTPUT_FILE = 'LiHua-World-merged.txt'\n",
    "\n",
    "# Merging the files and adding separators to make it easier for the model to distinguish between files\n",
    "def merge_txt_files(root_dir, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.txt'):\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                            content = infile.read()\n",
    "                            outfile.write('=== DOCUMENT START ===\\n')\n",
    "                            outfile.write(content)\n",
    "                            if not content.endswith('\\n'):\n",
    "                                outfile.write('\\n')  \n",
    "                            outfile.write('=== DOCUMENT END ===\\n\\n')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "merge_txt_files(ROOT_DIR, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18517265-1465-40f7-bb60-1787817e36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-generativeai\n",
    "%pip install -q requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15991aa7-1602-42a1-820f-23bff59b10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from google.generativeai import caching\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import types\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92862d49-5e26-4bb3-96af-cc379f6a65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"...\") # set the key\n",
    "config = types.GenerationConfig(temperature=0.0)\n",
    "\n",
    "# Load the large context text from file\n",
    "context_file_path = 'LiHua-World-merged.txt'  \n",
    "with open(context_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    context_text = f.read()\n",
    "\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are a domain expert assistant using ONLY the provided context to answer questions. \"\n",
    "    \"Always stick to the facts in the context and do not speculate.\"\n",
    "    \"Format your response as: Answer: <answer>. Evidence: <evidence>.\"\n",
    "    \"Give a concise answer to the user's question, and then quote the exact TIME as supporting evidence.\"\n",
    "    \"If the context doesn't provide necessary information, Answer 'Insufficient information' and Evidence: N/A\"\n",
    ")\n",
    "\n",
    "# Create the content payload for caching (the context is provided as a user message part)\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model=\"gemini-1.5-flash-001\",\n",
    "    system_instruction=system_instruction,\n",
    "    contents=context_text,\n",
    "    display_name=\"haystack-context-cache\",\n",
    "    ttl=datetime.timedelta(minutes=600), # time to live\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bbfd36-3f0c-464e-aa0c-9a7b7e252f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel.from_cached_content(cached_content=cached_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4437a1a-3ceb-44bd-a03c-3413c2b7e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the QA dataset from json file\n",
    "dataset_path = \"query_set.json\"  \n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_entries = json.load(f)\n",
    "\n",
    "# convert qa_entries values to a list\n",
    "if isinstance(qa_entries, dict):\n",
    "    qa_entries = list(qa_entries.values())\n",
    "\n",
    "print(f\"Total entries loaded: {len(qa_entries)}\")\n",
    "\n",
    "# Sample 100 random entries for evaluation\n",
    "random.seed(24)  \n",
    "sample_entries = random.sample(qa_entries, 100)\n",
    "print(f\"Sampled {len(sample_entries)} entries for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeddb7-3cbe-4441-a95d-3c15feb82425",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []  # tuples of (entry, model_answer)\n",
    "\n",
    "for entry in sample_entries:\n",
    "    question_text = entry[\"question\"]\n",
    "    # Call Gemini model with the question and cached context\n",
    "    try:\n",
    "        model_response = model.generate_content(question_text, generation_config=config)\n",
    "        answer_text = model_response.text.strip()\n",
    "    except Exception as e:\n",
    "        # Handle any API errors\n",
    "        print(f\"Error querying model for question: {question_text[:50]}... - {e}\")\n",
    "        answer_text = \"\"  # empty answer on failure\n",
    "    \n",
    "    # Store the entry and model answer for later evaluation\n",
    "    responses.append((entry, answer_text))\n",
    "    \n",
    "    # Delay for one second between API calls (was done to prevent the \"exceeded rate limit\" error)\n",
    "    time.sleep(3)\n",
    "    if len(responses)%10 == 0: \n",
    "        print(f\"{len(responses)}/{len(sample_entries)} queries processed\")\n",
    "\n",
    "# Check a sample output\n",
    "print(\"Example model response:\")\n",
    "print(f\"Q: {responses[0][0]['question']}\\nA: {responses[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c34e0-862b-470f-85f8-af4319f9603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all queries and responses to a json file\n",
    "output_data = []\n",
    "for entry, answer in responses:\n",
    "    output_data.append({\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"expected_answer\": entry[\"answer\"],\n",
    "        \"expected_evidence\": entry[\"evidence\"],\n",
    "        \"question_type\": entry[\"type\"],\n",
    "        \"model_answer\": answer\n",
    "    })\n",
    "\n",
    "output_file = \"model_responses.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"All queries and responses have been written to {output_file}\")\n",
    "\n",
    "\n",
    "# Display the first 10 sample responses\n",
    "print(\"\\nFirst 10 sample responses:\")\n",
    "for i, (entry, response_text) in enumerate(responses[:10], start=1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Question: {entry['question']}\")\n",
    "    print(f\"Model Answer: {response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20fb8f-5069-4ab7-88ae-ea2d64ecbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Helper function to normalize text for answer matching (ignore case and punctuation)\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Convert text to lowercase and remove punctuation and extra whitespace.\"\"\"\n",
    "    s = s.lower()\n",
    "    # Replace punctuation with space, then remove extra spaces\n",
    "    s = ''.join(ch if ch.isalnum() or ch.isspace() else ' ' for ch in s)\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "# Function to check if the model's answer contains the expected answer\n",
    "def check_answer(entry, model_output: str) -> bool:\n",
    "    target = entry[\"answer\"]\n",
    "    output_norm = normalize_text(model_output)\n",
    "    entry_type = entry[\"type\"].lower()\n",
    "    \n",
    "    if entry_type == \"single\":\n",
    "        # Single answer: the normalized target should be a substring of the normalized output.\n",
    "        target_norm = normalize_text(str(target))\n",
    "        return target_norm in output_norm\n",
    "    elif entry_type == \"multi\":\n",
    "        # Multi answer: check if any one of the target components appears in the output.\n",
    "        if isinstance(target, list):\n",
    "            target_components = [normalize_text(str(t)) for t in target]\n",
    "        elif isinstance(target, str):\n",
    "            # Split on common delimiters (comma, semicolon, slash, or ' and ')\n",
    "            parts = re.split(r',|;|/| and ', target)\n",
    "            target_components = [normalize_text(p) for p in parts if p.strip()]\n",
    "        else:\n",
    "            target_components = [normalize_text(str(target))]\n",
    "        return any(comp in output_norm for comp in target_components)\n",
    "    elif entry_type == \"null\":\n",
    "        # Null type: the expected correct answer is \"Insufficient information\".\n",
    "        expected = normalize_text(\"Insufficient information\")\n",
    "        return expected in output_norm\n",
    "    else:\n",
    "        # Fallback: if type is unrecognized, return False.\n",
    "        return False\n",
    "\n",
    "# Function to check if the model cited the exact evidence\n",
    "def check_evidence(entry, model_output: str) -> bool:\n",
    "    evidence_snippet = entry[\"evidence\"].strip()\n",
    "    return evidence_snippet in model_output\n",
    "\n",
    "# Evaluate each response\n",
    "score = 0\n",
    "incorrect_log = []  # to record details of incorrect responses\n",
    "for entry, model_ans in responses:\n",
    "    answer_ok = check_answer(entry, model_ans)\n",
    "    evidence_ok = check_evidence(entry, model_ans)\n",
    "    if answer_ok and evidence_ok: # might be practical to remove evidence as it's hard for the model to recite correctly, \n",
    "        # even when the answer itself is spot-on\n",
    "        score += 1  # correct on both counts\n",
    "    else:\n",
    "        # Log the incorrect case with relevant details\n",
    "        incorrect_log.append({\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"expected_answer\": entry[\"answer\"],\n",
    "            \"model_answer\": model_ans,\n",
    "            \"expected_evidence\": entry[\"evidence\"],\n",
    "            \"type\": entry[\"type\"]\n",
    "        })\n",
    "\n",
    "print(f\"Correctly answered: {score} out of {len(responses)}\")\n",
    "\n",
    "# Write the incorrect responses to a separate json file\n",
    "incorrect_output_file = \"incorrect_responses.json\"\n",
    "with open(incorrect_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(incorrect_log, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Incorrect responses saved to {incorrect_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
